Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 12
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	get_converted_data
	1	get_result_DEC
	2

[Tue Feb 18 11:38:57 2020]
rule get_converted_data:
    input: Original_Data/AirwayDiseasePhenotypingDataSets5/WSAS_AndOLIN_AirwayDiseasePhenotyping.csv, R_Functions/Functions.R
    output: Results/converted_data.csv
    jobid: 1

[Tue Feb 18 11:39:00 2020]
Finished job 1.
1 of 2 steps (50%) done

[Tue Feb 18 11:39:00 2020]
rule get_result_DEC:
    input: Results/converted_data.csv, Original_Data/AirwayDiseasePhenotypingDataSets5/WSAS_AndOLIN_AirwayDiseasePhenotyping.csv
    output: Results/result_airway_DEC.csv
    jobid: 0

[Tue Feb 18 11:39:08 2020]
Error in rule get_result_DEC:
    jobid: 0
    output: Results/result_airway_DEC.csv
    shell:
        python3 Python_Functions/code.py Results/converted_data.csv Original_Data/AirwayDiseasePhenotypingDataSets5/WSAS_AndOLIN_AirwayDiseasePhenotyping.csv > Results/result_airway_DEC.csv
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job get_result_DEC since they might be corrupted:
Results/result_airway_DEC.csv
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /Users/xbasra/Documents/Data/Airway_Clustering/.snakemake/log/2020-02-18T113857.117904.snakemake.log
